{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Copy of word2vec_lstmAttention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3mmPKQ48xdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "846fb840-eb64-4fb2-8cf9-8c06ac92eebd"
      },
      "source": [
        "!pip install rouge_score"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Collecting six>=1.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from rouge_score) (0.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rouge_score) (1.18.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge_score) (3.2.5)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: six, rouge-score\n",
            "  Found existing installation: six 1.12.0\n",
            "    Uninstalling six-1.12.0:\n",
            "      Successfully uninstalled six-1.12.0\n",
            "Successfully installed rouge-score-0.0.4 six-1.15.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfFL33q-8xdR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "868ecb23-cc4e-4534-f526-3316c3c9db8e"
      },
      "source": [
        "import requests       \n",
        "import json            \n",
        "import pandas as pd    \n",
        "import numpy as np    \n",
        "import matplotlib.pyplot as plt # for charts and such\n",
        "import json\n",
        "import nltk\n",
        "import csv\n",
        "from sklearn.utils import shuffle\n",
        "import pandas as pd\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from datetime import timedelta, datetime\n",
        "from dateutil import parser\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy.random as nr\n",
        "import tensorflow.keras as keras\n",
        "#import keras.utils.np_utils as ku\n",
        "import tensorflow.keras.models as models\n",
        "import tensorflow.keras.layers as layers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Dropout, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from keras.optimizers import rmsprop, Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import glob\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import Dense, Concatenate,Dropout, GRU, LSTM, Input, Activation, Conv1D, GlobalMaxPooling1D, Input,RepeatVector, Flatten,TimeDistributed,Embedding\n",
        "from google.colab import drive\n",
        "drive.mount('colabDrive')\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "path = 'colabDrive/My Drive/Colab Notebooks/LTP/data/'\n",
        "import pandas as pd\n",
        "import os\n",
        "from string import punctuation\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import io\n",
        "\n",
        "#vocab_size = 20000\n",
        "oov_tok = '<OOV>'\n",
        "max_length = 100\n",
        "def pre_process(data):\n",
        "  from string import punctuation\n",
        "  from nltk.stem.porter import PorterStemmer\n",
        "  processed_data = []\n",
        "  ps = PorterStemmer()\n",
        "  word_lemm = WordNetLemmatizer()\n",
        "  print(len(data))\n",
        "  for i in range(len(data)):\n",
        "    doc2Txt = str(data[i])\n",
        "\n",
        "    txt =  ''.join(c for c in doc2Txt if not c.isdigit())\n",
        "    txt = ''.join(c for c in txt if c not in punctuation).lower()\n",
        "    txt =  ' '.join([word_lemm.lemmatize(word) for word in txt.split()])\n",
        "    txt = ' '.join([word for word in txt.split() if word not in (stopwords.words('english'))])\n",
        "    processed_data.append(txt)\n",
        "    if i % 2000 == 0:\n",
        "      print('i = :' + str(i))\n",
        "  return processed_data\n",
        "\n",
        "def get_tokenize_sequence(data):\n",
        "  \n",
        "  print(vocab_size)\n",
        "  tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "  tokenizer.fit_on_texts(data)\n",
        "  word_index = tokenizer.word_index\n",
        "  \n",
        "  return tokenizer\n",
        "def loadProcessedSequence():\n",
        "    x_train = np.load('x_train_padded.npy')\n",
        "    y_train = np.load('y_train_padded.npy')\n",
        "    return x_train, y_train\n",
        "\n",
        "def get_tokenizer(data,oov_token ):\n",
        "  \n",
        "  tokenizer = Tokenizer(oov_token=oov_token)\n",
        "  tokenizer.fit_on_texts(data)\n",
        "  word_index = tokenizer.word_index\n",
        "  return tokenizer\n",
        "\n",
        "def addEosTokens(data):\n",
        "  for i in range(len(data)):\n",
        "    data[i] = \"<start> \" + data[i] + \" <eos>\"\n",
        "  return data\n",
        "def loadFiles(index):\n",
        "    #indexes from 1 to 7 \n",
        "    count = 0 \n",
        "    articles_ = np.array([])\n",
        "    for i in range(1,index+1):\n",
        "      articles_ = np.append(articles_,(np.load(path+'articles_lem_%d.npy'%i)))\n",
        "    \n",
        "    titles_ = np.load(path+'titles_lem_100k.npy')[:len(articles_)]\n",
        "\n",
        "    \n",
        "    return articles_,titles_\n",
        "def getVocabSize(data):\n",
        "  vocab = set()\n",
        "  for i in range(len(data)):\n",
        "    words = data[i].split()\n",
        "    for word in words:\n",
        "      vocab.add(word)\n",
        "  return len(vocab)\n",
        "\n",
        "def loadTokenizer(filename):    \n",
        "    with open(path+filename) as f:\n",
        "        data = json.load(f)\n",
        "        tokenizer_x = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
        "    \n",
        "   \n",
        "    return tokenizer_x\n",
        "\n",
        "def loadWord2vec(data, tok, vocab_size, latent_dim):\n",
        "    embedding_matrix = np.zeros(\n",
        "    (vocab_size, 100),\n",
        "    dtype='float32')\n",
        "    word2vec = gensim.models.Word2Vec(data, size=latent_dim, min_count=5, iter=10)\n",
        "    for word, i in tok.word_index.items():\n",
        "        if word in word2vec.wv.vocab:\n",
        "            if i < len(embedding_matrix):\n",
        "              embedding_matrix[i] = word2vec.wv.word_vec(word)\n",
        "    return word2vec, embedding_matrix\n",
        "def build_data():\n",
        "    # read data from the csv file (from the location it is stored)\n",
        "    Data = pd.read_csv('colabDrive/My Drive/Colab Notebooks/LTP/dataset/wikihowAll.csv')\n",
        "    Data = Data.astype(str)\n",
        "    rows, columns = Data.shape\n",
        "    summaries = []\n",
        "    articles = []\n",
        "    titles = []\n",
        "    \n",
        "    \n",
        "    for row in range(rows):\n",
        "\n",
        "        summary = Data.loc[row,'headline']\n",
        "                          # headline is the column representing the summary sentences\n",
        "        article = Data.loc[row,'text']           # text is the column representing the article\n",
        "        title = Data.loc[row,'title']\n",
        "        #  a threshold is used to remove short articles with long summaries as well as articles with no summary\n",
        "        if len(summary) < (0.75*len(article)) :\n",
        "            # remove extra commas in abstracts\n",
        "            summary = summary.replace(\".,\",\".\")\n",
        "            summary = summary.encode('utf-8')\n",
        "            title = title.replace(\".,\",\".\")\n",
        "            title = title.encode('utf-8')\n",
        "            # remove extra commas in articles\n",
        "            article = re.sub(r'[.]+[\\n]+[,]',\".\\n\", article)\n",
        "            article = article.encode('utf-8')\n",
        "            summaries.append(summary.decode())\n",
        "            articles.append(article.decode())\n",
        "            titles.append(title.decode())\n",
        "    return articles, titles\n",
        "def plot_accuracy(history):\n",
        "    train_acc = history['acc']\n",
        "    test_acc = history['val_acc']\n",
        "    x = list(range(1, len(test_acc) + 1))\n",
        "    plt.plot(x, test_acc, color = 'red', label = 'test accuracy')\n",
        "    plt.plot(x, train_acc, label = 'training accuracy')  \n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy vs. Epoch')  \n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss(history):\n",
        "    from pylab import rcParams\n",
        "    rcParams['figure.figsize'] = 5, 5\n",
        "\n",
        "    train_acc = history['loss']\n",
        "    test_acc = history['val_loss']\n",
        "    x = list(range(1, len(test_acc) + 1))\n",
        "    plt.plot(x, test_acc, color = 'red', label = 'test loss')\n",
        "    plt.plot(x, train_acc, label = 'training loss')  \n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Word2Vec Loss vs. Epoch')  \n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "print('loaded')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at colabDrive\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQdlgdfa5MVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoder inference\n",
        "latent_dim=100\n",
        "# encoder inference\n",
        "def GetEncoderDecoderInferenceModels(model):\n",
        "    latent_dim=100\n",
        "    encoder_inputs = model.input[0]  #loading encoder_inputs\n",
        "    encoder_outputs, state_h, state_c = model.layers[6].output #loading encoder_outputs\n",
        "    #print(encoder_outputs.shape)\n",
        "    encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "    # decoder inference\n",
        "    # Below tensors will hold the states of the previous time step\n",
        "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "    decoder_hidden_state_input = Input(shape=(max_encoder_seq_length,latent_dim))\n",
        "    # Get the embeddings of the decoder sequence\n",
        "    decoder_inputs = model.layers[3].output\n",
        "    #print(decoder_inputs.shape)\n",
        "    dec_emb_layer = model.layers[5]\n",
        "    dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "    # To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "    decoder_lstm = model.layers[7]\n",
        "    decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "    #attention inference\n",
        "    attn_layer = model.layers[8]\n",
        "    attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "    concate = model.layers[9]\n",
        "    decoder_inf_concat = concate([decoder_outputs2, attn_out_inf])\n",
        "    # A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "    decoder_dense = model.layers[10]\n",
        "    decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
        "    # Final decoder model\n",
        "    decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])\n",
        "    return encoder_model,decoder_model\n",
        "\n",
        "def decode_sequence(input_seq, encoderModel, decoderModel):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoderModel.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Chose the 'start' word as the first word of the target sequence\n",
        "    target_seq[0, 0] = tok_y.word_index['start']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoderModel.predict([target_seq] + [e_out, e_h, e_c])\n",
        "        # Sample a token\n",
        "        #print(output_tokens)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = ''\n",
        "        if sampled_token_index != 0:\n",
        "          sampled_token = reverse_word_index_y[sampled_token_index]\n",
        "\n",
        "        if(sampled_token!='eos'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "            # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'eos' or len(decoded_sentence.split()) >= (max_decoder_seq_length-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TZV2R0OKhQI",
        "colab_type": "text"
      },
      "source": [
        "Loading saved training data and test data \n",
        "Preparing input for the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tMGB8mi4BuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = np.load(path+'x_data.npy')\n",
        "y_data = np.load(path+'y_data.npy')\n",
        "idx = np.random.RandomState(seed=42).permutation(x_data.shape[0])\n",
        "test_size = 0.2 * len(x_data)\n",
        "test_size = int(test_size)\n",
        "train_idx = idx[test_size:]\n",
        "test_idx = idx[:test_size]\n",
        "x_train = x_data[train_idx]\n",
        "y_train = y_data[train_idx]\n",
        "x_test = x_data[test_idx]\n",
        "y_test = y_data[test_idx]\n",
        "embedding_x = np.load(path+'embedding_x.npy')\n",
        "embedding_y = np.load(path+'embedding_y.npy')\n",
        "tok_x = loadTokenizer('tokenizer_x.json')\n",
        "tok_y = loadTokenizer('tokenizer_y.json')\n",
        "reverse_word_index_x = dict([(value, key) for (key, value) in tok_x.word_index.items()])\n",
        "reverse_word_index_y = dict([(value, key) for (key, value) in tok_y.word_index.items()])\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2TKiK-dAiBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbtQ1yF-_gCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 100\n",
        "max_encoder_seq_length = 100\n",
        "max_decoder_seq_length = 10\n",
        "num_decoder_tokens = len(tok_x.word_index)+1\n",
        "num_encoder_tokens = len(tok_y.word_index)+1\n",
        "from sklearn.utils import shuffle\n",
        "epochs = 100\n",
        "import random\n",
        "x_vocab_size = len(tok_x.word_index)+1\n",
        "y_vocab_size = len(tok_y.word_index)+1\n",
        "\n",
        "encoder_input_data = Input(shape=(None, max_encoder_seq_length, None))\n",
        "\n",
        "decoder_input_data = Input(shape=(None, max_decoder_seq_length, num_decoder_tokens))\n",
        "#weights=[embedding_x] weights=[embedding_y]\n",
        "# Encoder \n",
        "encoder_inputs = Input(shape=(max_encoder_seq_length,)) \n",
        "enc_emb = Embedding(x_vocab_size , latent_dim,weights=[embedding_x] )(encoder_inputs) \n",
        "#kernel_regularizer='l2'\n",
        "#LSTM 1 \n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
        "\n",
        "#LSTM 2 \n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,kernel_regularizer='l2') \n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
        "\n",
        "#LSTM 3 \n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
        "# Set up the decoder. \n",
        "decoder_inputs = Input(shape=(None,)) \n",
        "dec_emb_layer = Embedding(y_vocab_size , latent_dim, weights=[embedding_y]) \n",
        "dec_emb = dec_emb_layer(decoder_inputs) \n",
        "\n",
        "#LSTM using encoder_states as initial state\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
        "\n",
        "#Attention Layer\n",
        "attn_layer = AttentionLayer(name='attention_layer') \n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
        "\n",
        "# Concat attention output and decoder LSTM output \n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "\n",
        "#Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(y_vocab_size, activation='softmax')) \n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "# Train \n",
        "\n",
        "\n",
        "sampleSize = 10000\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "epoch_index = 0\n",
        "epochs = 40\n",
        "from rouge_score import rouge_scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=False)\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "while True:\n",
        "    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "\n",
        "    history=model.fit([x_train,y_train[:,:-1]], y_train.reshape(y_train.shape[0],y_train.shape[1], 1)[:,1:],epochs = epochs, batch_size=2028,\n",
        "                 validation_data=([x_test,y_test[:,:-1]], y_test.reshape(y_test.shape[0],y_test.shape[1], 1)[:,1:]))\n",
        "    \n",
        "    epoch_index = epoch_index + epochs\n",
        "    model.save_weights(path+'model_weights%d.h5'%epoch_index)\n",
        "\n",
        "    percision = []\n",
        "    recall = []\n",
        "    \n",
        "    encoder, decoder = GetEncoderDecoderInferenceModels(model)\n",
        "    \n",
        "    ## print score for 20 test articles \n",
        "    for i in range (20):\n",
        "        predicted = decode_sequence(x_test[i].reshape(1,max_encoder_seq_length), encoder, decoder)\n",
        "        gold = y_test[i]\n",
        "        gold_output = []\n",
        "        scores = scorer.score(str(gold),\n",
        "                             str(predicted))\n",
        "        for j in range(len(gold)):\n",
        "          if gold[j] != 0:\n",
        "            gold_output.append(reverse_word_index_y[gold[j]])\n",
        "        print('************************************************************************************')\n",
        "        print(\"gold: \" + str(gold_output))\n",
        "        print(\"predicted: \" + predicted)\n",
        "        print('************************************************************************************')\n",
        "\n",
        "        percision.append(scores['rouge1'][0])\n",
        "        recall.append(scores['rouge1'][0])\n",
        "    print(percision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5Qp4Y8XJGII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(path+'baseline-model_weights80.h5')\n",
        "percision = []\n",
        "recall = []\n",
        "fscore = []\n",
        "\n",
        "percision2 = []\n",
        "recall2 = []\n",
        "fscore2 = []\n",
        "\n",
        "\n",
        "percisonL = []\n",
        "recallL = []\n",
        "fscoreL =[]\n",
        "\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.001), loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "encoder, decoder = GetEncoderDecoderInferenceModels(model)\n",
        "model.load_weights(path+'model_weights.h5')\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
        "from_epoch = 0\n",
        "\n",
        "for i in range (from_epoch,len(x_test)):\n",
        "        predicted = decode_sequence(x_test[i].reshape(1,max_encoder_seq_length), encoder, decoder)\n",
        "        gold = y_test[i]\n",
        "        gold_output = []\n",
        "        gold_str = ''                     \n",
        "        for j in range(len(gold)):\n",
        "          if gold[j] != 0:\n",
        "                gold_str = gold_str +  reverse_word_index_y[gold[j]] + \" \"\n",
        "\n",
        "       # print('************************************************************************************')\n",
        "       # print(\"gold: \" + str(gold_output))\n",
        "       # print(\"predicted: \" + predicted)\n",
        "       # print('************************************************************************************')\n",
        "        scores = scorer.score(gold_str,\n",
        "                          str(predicted))\n",
        "        percision.append(scores['rouge1'][0])\n",
        "        recall.append(scores['rouge1'][1])\n",
        "        fscore.append(scores['rouge1'][2])\n",
        "\n",
        "        percision2.append(scores['rouge2'][0])\n",
        "        recall2.append(scores['rouge2'][1])\n",
        "        fscore2.append(scores['rouge2'][2])\n",
        "\n",
        "\n",
        "        percisonL.append(scores['rougeL'][0])\n",
        "        recallL.append(scores['rougeL'][1])\n",
        "        fscoreL.append(scores['rougeL'][2])\n",
        "\n",
        "        if i % 1000 == 0 or i == len(x_test) - 1 :\n",
        "          file_name = path+\"baseline-score_%d_%d.txt\"%(from_epoch,i)\n",
        "          avgPercision = sum(percision)/len(percision)\n",
        "          avgRecall = sum(recall)/len(recall)\n",
        "          avgFScore = sum(fscore)/len(fscore)\n",
        "\n",
        "          avgPercision2 = sum(percision2)/len(percision2)\n",
        "          avgRecall2 = sum(recall2)/len(recall2)\n",
        "          avgFScore2 = sum(fscore2)/len(fscore2)\n",
        "\n",
        "          avgPercisionL = sum(percisonL)/len(percisonL)\n",
        "          avgRecallL = sum(recallL)/len(recallL)\n",
        "          avgFScoreL = sum(fscoreL)/len(fscoreL)\n",
        "\n",
        "          #np.savetxt(file_name,x)\n",
        "          #x = np.loadtxt(file_name)\n",
        "          print(i)\n",
        "          score1 = 'percision: %f, recall: %f, fscore: %f'%(avgPercision,avgRecall,avgFScore)\n",
        "          score2 = 'percision2: %f, recall2: %f, fscore2: %f'%(avgPercision2,avgRecall2,avgFScore2)\n",
        "          score3 = 'percision-L: %f, recall-L: %f, fscore-L: %f'%(avgPercisionL,avgRecallL,avgFScoreL)\n",
        "          print(score1)\n",
        "          print(score2)\n",
        "          print(score3)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}