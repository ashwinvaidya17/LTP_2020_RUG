{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "word2vec_lstmAttention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3mmPKQ48xdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "c4745630-0554-4208-bed1-6f46eee5709b"
      },
      "source": [
        "#!pip install google.colab\n",
        "!pip install rouge_score"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Collecting six>=1.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge_score) (3.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from rouge_score) (0.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rouge_score) (1.18.5)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: six, rouge-score\n",
            "  Found existing installation: six 1.12.0\n",
            "    Uninstalling six-1.12.0:\n",
            "      Successfully uninstalled six-1.12.0\n",
            "Successfully installed rouge-score-0.0.4 six-1.15.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfFL33q-8xdR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "fe639278-6523-4580-e14c-f9b3e2248f9f"
      },
      "source": [
        "import requests        # for making http requests to binance\n",
        "import json            # for parsing what binance sends back to us\n",
        "import pandas as pd    # for storing and manipulating the data we get back\n",
        "import numpy as np     # numerical python, i usually need this somewhere \n",
        "import matplotlib.pyplot as plt # for charts and such\n",
        "import json\n",
        "import nltk\n",
        "import csv\n",
        "from sklearn.utils import shuffle\n",
        "import pandas as pd\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords\n",
        "from datetime import timedelta, datetime\n",
        "from dateutil import parser\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy.random as nr\n",
        "import tensorflow.keras as keras\n",
        "#import keras.utils.np_utils as ku\n",
        "import tensorflow.keras.models as models\n",
        "import tensorflow.keras.layers as layers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Dropout, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from keras.optimizers import rmsprop, Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import glob\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import Dense, Concatenate,Dropout, GRU, LSTM, Input, Activation, Conv1D, GlobalMaxPooling1D, Input,RepeatVector, Flatten,TimeDistributed,Embedding\n",
        "from google.colab import drive\n",
        "drive.mount('colabDrive')\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "path = 'colabDrive/My Drive/Colab Notebooks/LTP/data/'\n",
        "import pandas as pd\n",
        "import os\n",
        "from string import punctuation\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "#vocab_size = 20000\n",
        "oov_tok = '<OOV>'\n",
        "max_length = 100\n",
        "def pre_process(data):\n",
        "  from string import punctuation\n",
        "  from nltk.stem.porter import PorterStemmer\n",
        "  processed_data = []\n",
        "  ps = PorterStemmer()\n",
        "  word_lemm = WordNetLemmatizer()\n",
        "  print(len(data))\n",
        "  for i in range(len(data)):\n",
        "    doc2Txt = str(data[i])\n",
        "\n",
        "    txt =  ''.join(c for c in doc2Txt if not c.isdigit())\n",
        "    txt = ''.join(c for c in txt if c not in punctuation).lower()\n",
        "    # Get the word stems\n",
        "    #txt =  ''.join( ps.stem(word) for word in txt)\n",
        "    txt =  ' '.join([word_lemm.lemmatize(word) for word in txt.split()])\n",
        "    txt = ' '.join([word for word in txt.split() if word not in (stopwords.words('english'))])\n",
        "    processed_data.append(txt)\n",
        "    if i % 2000 == 0:\n",
        "      print('i = :' + str(i))\n",
        "  return processed_data\n",
        "\n",
        "def get_tokenize_sequence(data):\n",
        "  \n",
        "  print(vocab_size)\n",
        "  tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "  tokenizer.fit_on_texts(data)\n",
        "  word_index = tokenizer.word_index\n",
        "  \n",
        "  return tokenizer\n",
        "def loadProcessedSequence():\n",
        "    x_train = np.load('x_train_padded.npy')\n",
        "    y_train = np.load('y_train_padded.npy')\n",
        "    return x_train, y_train\n",
        "\n",
        "def get_tokenizer(data,oov_token ):\n",
        "  \n",
        "  tokenizer = Tokenizer(oov_token=oov_token)\n",
        "  tokenizer.fit_on_texts(data)\n",
        "  word_index = tokenizer.word_index\n",
        "  return tokenizer\n",
        "\n",
        "def addEosTokens(data):\n",
        "  for i in range(len(data)):\n",
        "    data[i] = \"<start> \" + data[i] + \" <eos>\"\n",
        "  return data\n",
        "def loadFiles(index):\n",
        "    #indexes from 1 to 7 \n",
        "    count = 0 \n",
        "    articles_ = np.array([])\n",
        "    for i in range(1,index+1):\n",
        "      articles_ = np.append(articles_,(np.load(path+'articles_lem_%d.npy'%i)))\n",
        "    \n",
        "    titles_ = np.load(path+'titles_lem_100k.npy')[:len(articles_)]\n",
        "\n",
        "    \n",
        "    return articles_,titles_\n",
        "def getVocabSize(data):\n",
        "  vocab = set()\n",
        "  for i in range(len(data)):\n",
        "    words = data[i].split()\n",
        "    for word in words:\n",
        "      vocab.add(word)\n",
        "  return len(vocab)\n",
        "\n",
        "def loadTokenizer(filename):    \n",
        "    with open(path+filename) as f:\n",
        "        data = json.load(f)\n",
        "        tokenizer_x = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
        "    \n",
        "   \n",
        "    return tokenizer_x\n",
        "\n",
        "def loadWord2vec(data, tok, vocab_size, latent_dim):\n",
        "    embedding_matrix = np.zeros(\n",
        "    (vocab_size, 100),\n",
        "    dtype='float32')\n",
        "    word2vec = gensim.models.Word2Vec(data, size=latent_dim, min_count=5, iter=10)\n",
        "    for word, i in tok.word_index.items():\n",
        "        if word in word2vec.wv.vocab:\n",
        "            if i < len(embedding_matrix):\n",
        "              embedding_matrix[i] = word2vec.wv.word_vec(word)\n",
        "    return word2vec, embedding_matrix\n",
        "def build_data():\n",
        "    # read data from the csv file (from the location it is stored)\n",
        "    Data = pd.read_csv('colabDrive/My Drive/Colab Notebooks/LTP/dataset/wikihowAll.csv')\n",
        "    Data = Data.astype(str)\n",
        "    rows, columns = Data.shape\n",
        "    summaries = []\n",
        "    articles = []\n",
        "    titles = []\n",
        "    # create a file to record the file names. This can be later used to divide the dataset in train/dev/test sets\n",
        "    #title_file = open('titles.txt', 'wb')\n",
        "    \n",
        "    \n",
        "    for row in range(rows):\n",
        "\n",
        "        summary = Data.loc[row,'headline']\n",
        "                          # headline is the column representing the summary sentences\n",
        "        article = Data.loc[row,'text']           # text is the column representing the article\n",
        "        title = Data.loc[row,'title']\n",
        "        #  a threshold is used to remove short articles with long summaries as well as articles with no summary\n",
        "        if len(summary) < (0.75*len(article)) and len(articles) < 1100:\n",
        "            # remove extra commas in abstracts\n",
        "            summary = summary.replace(\".,\",\".\")\n",
        "            summary = summary.encode('utf-8')\n",
        "            title = title.replace(\".,\",\".\")\n",
        "            title = title.encode('utf-8')\n",
        "            # remove extra commas in articles\n",
        "            article = re.sub(r'[.]+[\\n]+[,]',\".\\n\", article)\n",
        "            article = article.encode('utf-8')\n",
        "            summaries.append(summary.decode())\n",
        "            articles.append(article.decode())\n",
        "            titles.append(title.decode())\n",
        "    return articles, titles\n",
        "def plot_accuracy(history):\n",
        "    train_acc = history['acc']\n",
        "    test_acc = history['val_acc']\n",
        "    x = list(range(1, len(test_acc) + 1))\n",
        "    plt.plot(x, test_acc, color = 'red', label = 'test accuracy')\n",
        "    plt.plot(x, train_acc, label = 'training accuracy')  \n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy vs. Epoch')  \n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss(history):\n",
        "    from pylab import rcParams\n",
        "    rcParams['figure.figsize'] = 5, 5\n",
        "\n",
        "    train_acc = history['loss']\n",
        "    test_acc = history['val_loss']\n",
        "    x = list(range(1, len(test_acc) + 1))\n",
        "    plt.plot(x, test_acc, color = 'red', label = 'test loss')\n",
        "    plt.plot(x, train_acc, label = 'training loss')  \n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Word2Vec Loss vs. Epoch')  \n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "print('loaded')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at colabDrive; to attempt to forcibly remount, call drive.mount(\"colabDrive\", force_remount=True).\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbHBQy48MlkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#json.dump(history_dict, open(path+'epoch-hist%d.json'%epoch_index, 'w'))\n",
        "#model.save_weights(path+'model_weights%d.h5'%epoch_index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svhl5OjGHzyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhB-HBWiDfzF",
        "colab_type": "text"
      },
      "source": [
        "Loading Saved train/test data and related files "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQdlgdfa5MVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoder inference\n",
        "latent_dim=100\n",
        "# encoder inference\n",
        "def GetEncoderDecoderInferenceModels(model):\n",
        "    latent_dim=100\n",
        "    encoder_inputs = model.input[0]  #loading encoder_inputs\n",
        "    encoder_outputs, state_h, state_c = model.layers[6].output #loading encoder_outputs\n",
        "    #print(encoder_outputs.shape)\n",
        "    encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "    # decoder inference\n",
        "    # Below tensors will hold the states of the previous time step\n",
        "    decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "    decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "    decoder_hidden_state_input = Input(shape=(max_encoder_seq_length,latent_dim))\n",
        "    # Get the embeddings of the decoder sequence\n",
        "    decoder_inputs = model.layers[3].output\n",
        "    #print(decoder_inputs.shape)\n",
        "    dec_emb_layer = model.layers[5]\n",
        "    dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "    # To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "    decoder_lstm = model.layers[7]\n",
        "    decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "    #attention inference\n",
        "    attn_layer = model.layers[8]\n",
        "    attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "    concate = model.layers[9]\n",
        "    decoder_inf_concat = concate([decoder_outputs2, attn_out_inf])\n",
        "    # A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "    decoder_dense = model.layers[10]\n",
        "    decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
        "    # Final decoder model\n",
        "    decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])\n",
        "    return encoder_model,decoder_model\n",
        "\n",
        "def decode_sequence(input_seq, encoderModel, decoderModel):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoderModel.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Chose the 'start' word as the first word of the target sequence\n",
        "    target_seq[0, 0] = tok_y.word_index['start']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoderModel.predict([target_seq] + [e_out, e_h, e_c])\n",
        "        # Sample a token\n",
        "        #print(output_tokens)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = ''\n",
        "        if sampled_token_index != 0:\n",
        "          sampled_token = reverse_word_index_y[sampled_token_index]\n",
        "\n",
        "        if(sampled_token!='eos'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "            # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'eos' or len(decoded_sentence.split()) >= (max_decoder_seq_length-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TZV2R0OKhQI",
        "colab_type": "text"
      },
      "source": [
        "Loading saved training data and test data \n",
        "Preparing input for the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tMGB8mi4BuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = np.load(path+'x_data.npy')\n",
        "y_data = np.load(path+'y_data.npy')\n",
        "idx = np.random.RandomState(seed=42).permutation(x_data.shape[0])\n",
        "test_size = 0.2 * len(x_data)\n",
        "test_size = int(test_size)\n",
        "train_idx = idx[test_size:]\n",
        "test_idx = idx[:test_size]\n",
        "x_train = x_data[train_idx]\n",
        "y_train = y_data[train_idx]\n",
        "x_test = x_data[test_idx]\n",
        "y_test = y_data[test_idx]\n",
        "embedding_x = np.load(path+'embedding_x.npy')\n",
        "embedding_y = np.load(path+'embedding_y.npy')\n",
        "tok_x = loadTokenizer('tokenizer_x.json')\n",
        "tok_y = loadTokenizer('tokenizer_y.json')\n",
        "reverse_word_index_x = dict([(value, key) for (key, value) in tok_x.word_index.items()])\n",
        "reverse_word_index_y = dict([(value, key) for (key, value) in tok_y.word_index.items()])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xugj8TRa5DbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok_y = get_tokenizer(titles,oov_token=oov_tok )\n",
        "y_seq = tok_y.texts_to_sequences(titles)\n",
        "y_data = pad_sequences(y_seq, maxlen=10)\n",
        "np.save('colabDrive/My Drive/Colab Notebooks/LTP/data/y_data.npy',y_data)\n",
        "import io\n",
        "tokenizer_json = tok_y.to_json()\n",
        "with io.open('colabDrive/My Drive/Colab Notebooks/LTP/data/tokenizer_y.json', 'w', encoding='utf-8') as f:\n",
        "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
        "w2v, embedding_y = loadWord2vec(titles, tok_y, len(tok_y.word_index)+1, 100)\n",
        "np.save('colabDrive/My Drive/Colab Notebooks/LTP/data/embedding_y.npy',embedding_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2TKiK-dAiBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbtQ1yF-_gCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 100\n",
        "max_encoder_seq_length = 100\n",
        "max_decoder_seq_length = 10\n",
        "num_decoder_tokens = len(tok_x.word_index)+1\n",
        "num_encoder_tokens = len(tok_y.word_index)+1\n",
        "from sklearn.utils import shuffle\n",
        "epochs = 100\n",
        "import random\n",
        "x_vocab_size = len(tok_x.word_index)+1\n",
        "y_vocab_size = len(tok_y.word_index)+1\n",
        "\n",
        "encoder_input_data = Input(shape=(None, max_encoder_seq_length, None))\n",
        "\n",
        "decoder_input_data = Input(shape=(None, max_decoder_seq_length, num_decoder_tokens))\n",
        "#weights=[embedding_x] weights=[embedding_y]\n",
        "# Encoder \n",
        "encoder_inputs = Input(shape=(max_encoder_seq_length,)) \n",
        "enc_emb = Embedding(x_vocab_size , latent_dim,trainable=True)(encoder_inputs) \n",
        "#kernel_regularizer='l2'\n",
        "#LSTM 1 \n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
        "\n",
        "#LSTM 2 \n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,kernel_regularizer='l2') \n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
        "#dropoutLayer = Dropout(0.2)\n",
        "#drop_output = dropoutLayer(encoder_output2)\n",
        "#LSTM 3 \n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
        "# Set up the decoder. \n",
        "decoder_inputs = Input(shape=(None,)) \n",
        "dec_emb_layer = Embedding(y_vocab_size , latent_dim,trainable=True) \n",
        "dec_emb = dec_emb_layer(decoder_inputs) \n",
        "\n",
        "#LSTM using encoder_states as initial state\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
        "\n",
        "#Attention Layer\n",
        "attn_layer = AttentionLayer(name='attention_layer') \n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
        "\n",
        "# Concat attention output and decoder LSTM output \n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "#temp = Dense(units=100,activation='relu')\n",
        "##temp_outputs = temp(decoder_concat_input)\n",
        "\n",
        "#Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(y_vocab_size, activation='softmax')) \n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.summary()\n",
        "# Train \n",
        "\n",
        "\n",
        "sampleSize = 10000\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "epoch_index = 0\n",
        "epochs = 0\n",
        "#model.load_weights(path+'model_weights120.h5')\n",
        "from rouge_score import rouge_scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=False)\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "while True:\n",
        "    model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.001), loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "\n",
        "    history=model.fit([x_train,y_train[:,:-1]], y_train.reshape(y_train.shape[0],y_train.shape[1], 1)[:,1:],epochs = epochs, batch_size=1024,\n",
        "                 validation_data=([x_test,y_test[:,:-1]], y_test.reshape(y_test.shape[0],y_test.shape[1], 1)[:,1:]))\n",
        "\n",
        "    epoch_index = epoch_index + epochs\n",
        "    history_dict = history.history\n",
        "\n",
        "    percision = []\n",
        "    recall = []\n",
        "    \n",
        "    encoder, decoder = GetEncoderDecoderInferenceModels(model)\n",
        "    #index = random.randint(1,1000)\n",
        "    \n",
        "    ## print score for 100 test articles \n",
        "    for i in range (100):\n",
        "        predicted = decode_sequence(x_test[i].reshape(1,max_encoder_seq_length), encoder, decoder)\n",
        "        gold = y_test[i]\n",
        "        gold_output = []\n",
        "        scores = scorer.score(str(gold),\n",
        "                             str(predicted))\n",
        "        for j in range(len(gold)):\n",
        "          if gold[j] != 0:\n",
        "            gold_output.append(reverse_word_index_y[gold[j]])\n",
        "        print('************************************************************************************')\n",
        "        print(\"gold: \" + str(gold_output))\n",
        "        print(\"predicted: \" + predicted)\n",
        "        print('************************************************************************************')\n",
        "\n",
        "        percision.append(scores['rouge1'][0])\n",
        "        recall.append(scores['rouge1'][0])\n",
        "        index = index + 10\n",
        "    print(percision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5Qp4Y8XJGII",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "3b424b69-f870-4e91-b60f-ac478b2cefbb"
      },
      "source": [
        "model.load_weights(path+'baseline-model_weights80.h5')\n",
        "percision = []\n",
        "recall = []\n",
        "percisonL = []\n",
        "recallL = []\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.001), loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "encoder, decoder = GetEncoderDecoderInferenceModels(model)\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=False)\n",
        "from_epoch = 0\n",
        "\n",
        "for i in range (from_epoch,len(x_test)):\n",
        "        index = random.randint(0,len(x_test)-1)\n",
        "        predicted = decode_sequence(x_test[i].reshape(1,max_encoder_seq_length), encoder, decoder)\n",
        "        gold = y_test[i]\n",
        "        gold_output = []\n",
        "        gold_str = ''                     \n",
        "        for j in range(len(gold)):\n",
        "          if gold[j] != 0:\n",
        "                gold_str = gold_str +  reverse_word_index_y[gold[j]] + \" \"\n",
        "\n",
        "       # print('************************************************************************************')\n",
        "       # print(\"gold: \" + str(gold_output))\n",
        "       # print(\"predicted: \" + predicted)\n",
        "       # print('************************************************************************************')\n",
        "        scores = scorer.score(gold_str,\n",
        "                          str(predicted))\n",
        "        percision.append(scores['rouge1'][0])\n",
        "        recall.append(scores['rouge1'][1])\n",
        "        percisonL.append(scores['rougeL'][0])\n",
        "        recallL.append(scores['rougeL'][1])\n",
        "        if i % 1000 == 0 or i == len(x_test) - 1 :\n",
        "          file_name = path+\"baseline-score_%d_%d.txt\"%(from_epoch,i)\n",
        "          avgPercision = sum(percision)/len(percision)\n",
        "          avgRecall = sum(recall)/len(recall)\n",
        "          avgPercisionL = sum(percisonL)/len(percisonL)\n",
        "          avgRecallL = sum(recallL)/len(recallL)\n",
        "          x = (avgPercision,avgRecall,avgPercisionL,avgRecallL,i)\n",
        "          print(x)\n",
        "          np.savetxt(file_name,x)\n",
        "          #x = np.loadtxt(file_name)\n",
        "\n",
        "        score = \"percision: %f, recall: %f,percisionL: %f, recallL: %f , at index %d \"%(avgPercision,avgRecall,avgPercisionL,avgRecallL,i)\n",
        "          \n",
        "#print(sum(percision)/len(percision),  sum(recall)/len(recall), sum(percisonL)/len(percisonL),sum(recallL)/len(recallL))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0.125, 0.1, 0.125, 0.1, 0)\n",
            "(0.33303521874950454, 0.31790356108537904, 0.330551591265877, 0.31575174464784833, 1000)\n",
            "(0.3303386005409994, 0.3163278389665199, 0.3277753980152781, 0.314089474022008, 2000)\n",
            "(0.331241305808117, 0.31630145651305475, 0.3280823058980341, 0.3135756455399931, 3000)\n",
            "(0.3311507440600163, 0.31617869342188004, 0.3276788937686209, 0.31318410476745656, 4000)\n",
            "(0.3308527183452192, 0.31596827099225977, 0.32745220797110347, 0.31302933496994023, 5000)\n",
            "(0.3298592430362126, 0.31539205346486493, 0.3265046434059922, 0.3124886352514979, 6000)\n",
            "(0.3311827592965994, 0.3165299051279533, 0.3278649226156717, 0.3136553724648182, 7000)\n",
            "(0.33102471588598265, 0.31667464727081496, 0.32777874266438134, 0.3138684504263723, 8000)\n",
            "(0.33067390459902357, 0.3160376440456371, 0.3274131557933353, 0.31322014405004517, 9000)\n",
            "(0.33021826605218235, 0.31568393665683503, 0.3270335845203355, 0.3129351242364893, 10000)\n",
            "(0.330132269885474, 0.3159046737585025, 0.32687185200138547, 0.3130907881521428, 11000)\n",
            "(0.3292804784305492, 0.31508172479133056, 0.32601051283244425, 0.3122509752373907, 12000)\n",
            "(0.329439446493904, 0.31513511892136503, 0.32613255801279806, 0.31227297644880764, 13000)\n",
            "(0.3294814388129156, 0.31522746313832817, 0.3261696855708001, 0.31235881625853895, 14000)\n",
            "(0.3290647939488026, 0.31487340237923916, 0.3256914209567374, 0.31194758973740755, 15000)\n",
            "(0.32903420854017806, 0.31505204445646223, 0.3256604858660991, 0.3121210249936634, 16000)\n",
            "(0.32862232320940754, 0.31480568836754486, 0.3252752511884339, 0.31190097637072073, 17000)\n",
            "(0.3286494986289457, 0.31486085870009883, 0.3252711148883598, 0.31193855031142764, 18000)\n",
            "(0.3287859422299731, 0.31508872700784657, 0.32531031062161164, 0.31208035834372344, 19000)\n",
            "(0.328510468415975, 0.3148081683217534, 0.32499572351829914, 0.3117655372642736, 20000)\n",
            "(0.3287047830402927, 0.3149797632120024, 0.3252138872930757, 0.31195677275865213, 21000)\n",
            "(0.3288936082729566, 0.3150654407483772, 0.3253322261424184, 0.31198045700796945, 22000)\n",
            "(0.3287246388622437, 0.3148364416428984, 0.32518473065949466, 0.3117631406075529, 23000)\n",
            "(0.3287787221227518, 0.3149696128582568, 0.3252129480776226, 0.3118699359147825, 24000)\n",
            "(0.3287986518057339, 0.3150464356606262, 0.32524534949338196, 0.3119555549668504, 25000)\n",
            "(0.32892307311079916, 0.31525739963493915, 0.325356558880272, 0.3121564616257293, 26000)\n",
            "(0.3289345491078788, 0.31523003423582857, 0.3253735845815796, 0.3121374199010394, 27000)\n",
            "(0.32905592992375854, 0.31524369646045663, 0.3254836085270738, 0.3121448661929551, 28000)\n",
            "(0.32911872273101433, 0.31529285475703, 0.3255522474644254, 0.3121982122525409, 29000)\n",
            "(0.32907971369689526, 0.3153339862216418, 0.3255153219279438, 0.3122387154194629, 30000)\n",
            "(0.32906370194361123, 0.31539848153668976, 0.3255002352249238, 0.31230843435532457, 31000)\n",
            "(0.32912146887725974, 0.3154760540717423, 0.325567797687421, 0.3123967322359768, 32000)\n",
            "(0.32920003670359926, 0.3155810628940427, 0.32566681043772866, 0.3125155801173404, 33000)\n",
            "(0.32897247462334, 0.3154726244576747, 0.32547331283398095, 0.3124330514767778, 34000)\n",
            "(0.32906855310215427, 0.31556570966661335, 0.32556976417866335, 0.3125228796802687, 35000)\n",
            "(0.32890963165886294, 0.31550760194133387, 0.32544553035303514, 0.31249327569987667, 36000)\n",
            "(0.3289091089340937, 0.315500996882698, 0.3254473154264988, 0.31248867879751546, 36024)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}